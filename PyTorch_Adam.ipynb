{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in my Windows the optimizers are located in\n",
    "#/c/Users/zb0857/AppData/Local/Continuum/anaconda2/envs/Tensorflow/lib/site-packages/tensorflow/python/keras\n",
    "#C:\\Users\\zb0857\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\Lib\\site-packages\\tensorflow\\python\\keras\n",
    "#/c/Users/zb0857/AppData/Local/Continuum/anaconda2/envs/Tensorflow/lib/site-packages/sklearn/neural_network\n",
    "#C:\\Users\\zb0857\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\Lib\\site-packages\\sklearn\\neural_network\n",
    "#rememmber to start the notebook using the environment tensorflow\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "import numpy as np\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.keras.optimizers import Fire\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#it seems that cateorical cross-entropy is not available in pytorch. We have to make or own\n",
    "#    implementation of the loss function\n",
    "#https://datascience.stackexchange.com/questions/55962/pytorch-doing-a-cross-entropy-loss-when-the-predictions-already-have-probabiliti\n",
    "def categorical_cross_entropy(y_pred, y_true):\n",
    "    y_pred = torch.clamp(y_pred, 1e-9, 1 - 1e-9)\n",
    "    return -(y_true * torch.log(y_pred)).sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropyLoss\n",
    "$\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)$\n",
    "                       $= -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)$\n",
    "                       \n",
    "                       \n",
    "        nll_loss(log_softmax(input, 1),\n",
    "\n",
    "\n",
    "### Softmax\n",
    "$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the input data\n",
    "\n",
    "#loading the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "#Separating into train and test (60000 train, 10000 test)\n",
    "(x_train, y_train0),(x_test, y_test0) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#flattening the images (from 28x28 to 784)\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "\n",
    "#converting the otputs (labels) into one hot vectors\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.array([0,1,2,3,4,5,6,7,8,9]))\n",
    "y_train = lb.transform(y_train0)\n",
    "y_test = lb.transform(y_test0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the input parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the input parameters\n",
    "BatchSize         = 5001\n",
    "NeuronsLayer1     = 100\n",
    "NeuronsLayer2     = 100\n",
    "Epochs            = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch by me\n",
    "https://stackoverflow.com/questions/51041128/pytorch-predict-single-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensors to hold inputs and outputs\n",
    "\n",
    "torch_X_train = torch.from_numpy(x_train)\n",
    "torch_y_train = torch.from_numpy(y_train)\n",
    "\n",
    "#the trainloader helps us deal with the batches\n",
    "train = torch.utils.data.TensorDataset(torch_X_train.float(),torch_y_train.float())\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BatchSize, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the net\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,NeuronsLayer1)\n",
    "        self.linear2 = nn.Linear(NeuronsLayer1,NeuronsLayer2)\n",
    "        self.linear3 = nn.Linear(NeuronsLayer2,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = F.softmax(self.linear3(X), dim=1)\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above.\n",
    "model = MLP()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. \n",
    "\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# loss_fn = torch.nn.NLLLoss()\n",
    "# loss_fn = categorical_cross_entropy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MLP(\n",
       "  (linear1): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show aprameters of the network\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.850341796875 2.121517493289709\n",
      "2 1.0361303091049194 1.4550372563242913\n",
      "3 0.5268484950065613 0.7938365449965\n",
      "4 0.356965571641922 0.5048875432923436\n",
      "5 0.29270148277282715 0.4006705708324909\n",
      "6 0.25832346081733704 0.3512985180899501\n",
      "7 0.2352951169013977 0.3205249663233757\n",
      "8 0.2181140035390854 0.29769819027110933\n",
      "9 0.2041122019290924 0.2789956733778119\n",
      "10 0.19231396913528442 0.26285469793751837\n",
      "11 0.18144388496875763 0.24817762532830237\n",
      "12 0.17134718596935272 0.2345243138000369\n",
      "13 0.1618925780057907 0.22164313690960408\n",
      "14 0.1532825231552124 0.2097093479819596\n",
      "15 0.14553293585777283 0.19860787144228817\n",
      "16 0.13840030133724213 0.18846724583879113\n",
      "17 0.13191820681095123 0.17926902678906917\n",
      "18 0.12607821822166443 0.1708042189307511\n",
      "19 0.12064962089061737 0.16299815270379187\n",
      "20 0.11586832255125046 0.15587103875987232\n",
      "21 0.11133965104818344 0.14930231818668543\n",
      "22 0.10712460428476334 0.14317228548787533\n",
      "23 0.10316768288612366 0.13745868683904408\n",
      "24 0.09963034093379974 0.1321375793594867\n",
      "25 0.09621120244264603 0.1271846991445869\n",
      "26 0.0931774154305458 0.12252304392457009\n",
      "27 0.09018540382385254 0.11812444403469563\n",
      "28 0.08740302175283432 0.11394696353152395\n",
      "29 0.08479179441928864 0.11001488833539187\n",
      "30 0.08228593319654465 0.10623228184655309\n",
      "31 0.0798717737197876 0.10268125619702041\n",
      "32 0.07771289348602295 0.09928871857002378\n",
      "33 0.07568078488111496 0.09607445631660522\n",
      "34 0.07368636131286621 0.0930418257817626\n",
      "35 0.07169982045888901 0.09011143297553062\n",
      "36 0.06971168518066406 0.08735477309674025\n",
      "37 0.0678485780954361 0.08471757098808885\n",
      "38 0.06591025739908218 0.08218709881678224\n",
      "39 0.06391959637403488 0.0797490595575422\n",
      "40 0.061970870941877365 0.07739815859962254\n",
      "41 0.05999457463622093 0.07512198351640255\n",
      "42 0.05809828266501427 0.07295914898496121\n",
      "43 0.05619305372238159 0.07080843237638473\n",
      "44 0.05436665937304497 0.06874154386948794\n",
      "45 0.052607856690883636 0.06671212575994431\n",
      "46 0.05092081427574158 0.0647052910817787\n",
      "47 0.0494396910071373 0.06275576138794423\n",
      "48 0.04797205328941345 0.06085460504554212\n",
      "49 0.04653346538543701 0.059017667012102905\n",
      "50 0.04518411308526993 0.05727406353186816\n",
      "51 0.04384872689843178 0.055588341217674316\n",
      "52 0.04261215403676033 0.053968407756276426\n",
      "53 0.041399043053388596 0.05241286915559322\n",
      "54 0.04027346521615982 0.050907338798977436\n",
      "55 0.03911590203642845 0.049495602564886214\n",
      "56 0.03791169449687004 0.04810616418216378\n",
      "57 0.03686511144042015 0.04681915062125772\n",
      "58 0.03578667342662811 0.04559717827346176\n",
      "59 0.03480127826333046 0.044404362484440206\n",
      "60 0.033776748925447464 0.04327192592062056\n",
      "61 0.03275793045759201 0.042170458089001474\n",
      "62 0.03182687610387802 0.04114933676663786\n",
      "63 0.030860016122460365 0.040116809914354234\n",
      "64 0.029910089448094368 0.0392095857786946\n",
      "65 0.029033970087766647 0.03829946681149304\n",
      "66 0.028183622285723686 0.03745322711868212\n",
      "67 0.02744385041296482 0.03667182549489662\n",
      "68 0.0267488956451416 0.035929073300771414\n",
      "69 0.026258721947669983 0.0352547311459668\n",
      "70 0.025898896157741547 0.034610337168257684\n",
      "71 0.02580457367002964 0.034087893683835864\n",
      "72 0.02591967210173607 0.033595084168575706\n",
      "73 0.026194406673312187 0.033116364710591734\n",
      "74 0.026507942005991936 0.0326787424935028\n",
      "75 0.02639681287109852 0.03216291437540204\n",
      "76 0.025453628972172737 0.031651975107565525\n",
      "77 0.023455113172531128 0.031152018563821913\n",
      "78 0.021195916458964348 0.030579420448467134\n",
      "79 0.019746676087379456 0.029676830450072883\n",
      "80 0.019655918702483177 0.028327633500285446\n",
      "81 0.020298629999160767 0.026622406128514558\n",
      "82 0.020140280947089195 0.024846695977728813\n",
      "83 0.01886843517422676 0.023315755678806453\n",
      "84 0.01743871532380581 0.022203800817672162\n",
      "85 0.01661919429898262 0.02145508735049516\n",
      "86 0.016123270615935326 0.020861175302881747\n",
      "87 0.015762371942400932 0.02037552213454619\n",
      "88 0.015402089804410934 0.019886215620487927\n",
      "89 0.01500080619007349 0.019413577575189994\n",
      "90 0.01473335362970829 0.018935586939519272\n",
      "91 0.014359432272613049 0.018476483679004015\n",
      "92 0.014105278998613358 0.018014337367098777\n",
      "93 0.01376268733292818 0.017582339948788286\n",
      "94 0.013486342504620552 0.017156128544639795\n",
      "95 0.013136209920048714 0.016765478275343776\n",
      "96 0.01289456244558096 0.016384562932606785\n",
      "97 0.012627847492694855 0.01606038909899071\n",
      "98 0.012362630106508732 0.015726055930461735\n",
      "99 0.012226305902004242 0.01543899046331644\n",
      "100 0.011967508122324944 0.015155287863314151\n"
     ]
    }
   ],
   "source": [
    "#train the network\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    loss_avg = 0.0\n",
    "    N_elements = 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = categorical_cross_entropy(y_pred, y_batch)\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_avg += loss.item()*X_batch.size()[0]\n",
    "        N_elements += X_batch.size()[0]\n",
    "\n",
    "    # print loss after each epoch    \n",
    "    print(epoch+1, loss.item(), loss_avg/N_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01680797794812698\n"
     ]
    }
   ],
   "source": [
    "#Predicting\n",
    "y_train_hat_Pytorch = model(torch_X_train.float()).detach().numpy()\n",
    "\n",
    "print(log_loss(y_train, y_train_hat_Pytorch))\n",
    "# print(categorical_cross_entropy(torch.from_numpy(y_train).float(), torch.from_numpy(y_train_hat_Pytorch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6251083e-16 1.0013642e-09 2.1500260e-12 2.6722231e-03 1.0064816e-22\n",
      " 9.9732780e-01 4.1204474e-20 6.0437831e-12 3.5702879e-15 2.8144921e-13]\n",
      "[9.9999523e-01 1.5031657e-14 4.7886738e-06 1.8797798e-10 2.6785288e-13\n",
      " 8.6480465e-13 5.7663790e-10 3.5290895e-09 2.2213843e-12 2.8939884e-09]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_hat_Pytorch[0])\n",
    "print(y_train_hat_Pytorch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 100]          78,500\n",
      "            Linear-2               [-1, 1, 100]          10,100\n",
      "            Linear-3                [-1, 1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.34\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Show aprameters of the network\n",
    "\n",
    "summary(model, (1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

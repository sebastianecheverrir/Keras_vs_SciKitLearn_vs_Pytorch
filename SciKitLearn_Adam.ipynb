{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in my Windows the optimizers are located in\n",
    "#/c/Users/zb0857/AppData/Local/Continuum/anaconda2/envs/Tensorflow/lib/site-packages/tensorflow/python/keras\n",
    "#C:\\Users\\zb0857\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\Lib\\site-packages\\tensorflow\\python\\keras\n",
    "#/c/Users/zb0857/AppData/Local/Continuum/anaconda2/envs/Tensorflow/lib/site-packages/sklearn/neural_network\n",
    "#C:\\Users\\zb0857\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\Lib\\site-packages\\sklearn\\neural_network\n",
    "#rememmber to start the notebook using the environment tensorflow\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "import numpy as np\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.keras.optimizers import Fire\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the input data\n",
    "\n",
    "#loading the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "#Separating into train and test (60000 train, 10000 test)\n",
    "(x_train, y_train0),(x_test, y_test0) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#flattening the images (from 28x28 to 784)\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "#we do not need to convert the outputs to one hot vectors.\n",
    "#Also, if we convert the output to one hot vectors, the _label_binarizer.y_type_ \n",
    "#    becomes multilabel and the out_activation_ becomes logistic\n",
    "#    Since keras and pytorch use softmax, we need to keep it like this\n",
    "# #converting the otputs (labels) into one hot vectors\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "# lb.fit(np.array([0,1,2,3,4,5,6,7,8,9]))\n",
    "# y_train = lb.transform(y_train0)\n",
    "# y_test = lb.transform(y_test0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the input parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the input parameters\n",
    "BatchSize         = 5001\n",
    "NeuronsLayer1     = 100\n",
    "NeuronsLayer2     = 100\n",
    "Epochs            = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model in SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the net\n",
    "# Construct our model, # Construct our loss function and an Optimizer. \n",
    "\n",
    "#MLP regressor uses the square loss\n",
    "#MLP classifier uses the log loss function\n",
    "MLP_clf =  MLPClassifier( hidden_layer_sizes         = (NeuronsLayer1,NeuronsLayer2),\n",
    "                          activation                 = 'relu',#,'identity', 'tanh', 'relu'],\n",
    "                          solver                     = 'adam',\n",
    "                          alpha                      = 0.0         ,\n",
    "                          batch_size                 = BatchSize,\n",
    "                          max_iter                   = Epochs              ,\n",
    "                          random_state               = 1234,\n",
    "                          tol                        = -10.0 ,\n",
    "                          verbose                    = True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0,\n",
       " 'batch_size': 5001,\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100, 100),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_iter': 100,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1234,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': -10.0,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': True,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show aprameters of the network\n",
    "MLP_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.81390930\n",
      "Iteration 2, loss = 0.89289847\n",
      "Iteration 3, loss = 0.49292538\n",
      "Iteration 4, loss = 0.36625217\n",
      "Iteration 5, loss = 0.30977199\n",
      "Iteration 6, loss = 0.27606573\n",
      "Iteration 7, loss = 0.25161308\n",
      "Iteration 8, loss = 0.23266756\n",
      "Iteration 9, loss = 0.21603850\n",
      "Iteration 10, loss = 0.20175424\n",
      "Iteration 11, loss = 0.18910124\n",
      "Iteration 12, loss = 0.17756110\n",
      "Iteration 13, loss = 0.16786668\n",
      "Iteration 14, loss = 0.15873063\n",
      "Iteration 15, loss = 0.15019650\n",
      "Iteration 16, loss = 0.14267300\n",
      "Iteration 17, loss = 0.13554877\n",
      "Iteration 18, loss = 0.12945584\n",
      "Iteration 19, loss = 0.12341745\n",
      "Iteration 20, loss = 0.11825523\n",
      "Iteration 21, loss = 0.11312220\n",
      "Iteration 22, loss = 0.10818262\n",
      "Iteration 23, loss = 0.10334975\n",
      "Iteration 24, loss = 0.09879028\n",
      "Iteration 25, loss = 0.09429546\n",
      "Iteration 26, loss = 0.09112211\n",
      "Iteration 27, loss = 0.08690867\n",
      "Iteration 28, loss = 0.08280716\n",
      "Iteration 29, loss = 0.08051790\n",
      "Iteration 30, loss = 0.07726292\n",
      "Iteration 31, loss = 0.07414604\n",
      "Iteration 32, loss = 0.07088575\n",
      "Iteration 33, loss = 0.06906936\n",
      "Iteration 34, loss = 0.06593593\n",
      "Iteration 35, loss = 0.06347654\n",
      "Iteration 36, loss = 0.06180981\n",
      "Iteration 37, loss = 0.05965035\n",
      "Iteration 38, loss = 0.05737602\n",
      "Iteration 39, loss = 0.05582850\n",
      "Iteration 40, loss = 0.05360098\n",
      "Iteration 41, loss = 0.05204030\n",
      "Iteration 42, loss = 0.05016450\n",
      "Iteration 43, loss = 0.04875742\n",
      "Iteration 44, loss = 0.04676229\n",
      "Iteration 45, loss = 0.04549349\n",
      "Iteration 46, loss = 0.04399893\n",
      "Iteration 47, loss = 0.04273549\n",
      "Iteration 48, loss = 0.04106828\n",
      "Iteration 49, loss = 0.03942955\n",
      "Iteration 50, loss = 0.03832312\n",
      "Iteration 51, loss = 0.03725964\n",
      "Iteration 52, loss = 0.03563360\n",
      "Iteration 53, loss = 0.03511451\n",
      "Iteration 54, loss = 0.03387426\n",
      "Iteration 55, loss = 0.03258376\n",
      "Iteration 56, loss = 0.03171806\n",
      "Iteration 57, loss = 0.03083352\n",
      "Iteration 58, loss = 0.02971702\n",
      "Iteration 59, loss = 0.02909394\n",
      "Iteration 60, loss = 0.02791311\n",
      "Iteration 61, loss = 0.02718363\n",
      "Iteration 62, loss = 0.02600628\n",
      "Iteration 63, loss = 0.02505097\n",
      "Iteration 64, loss = 0.02416748\n",
      "Iteration 65, loss = 0.02367502\n",
      "Iteration 66, loss = 0.02298435\n",
      "Iteration 67, loss = 0.02216787\n",
      "Iteration 68, loss = 0.02128289\n",
      "Iteration 69, loss = 0.02122907\n",
      "Iteration 70, loss = 0.02037187\n",
      "Iteration 71, loss = 0.01959205\n",
      "Iteration 72, loss = 0.01900317\n",
      "Iteration 73, loss = 0.01825216\n",
      "Iteration 74, loss = 0.01782987\n",
      "Iteration 75, loss = 0.01720556\n",
      "Iteration 76, loss = 0.01648360\n",
      "Iteration 77, loss = 0.01582749\n",
      "Iteration 78, loss = 0.01550714\n",
      "Iteration 79, loss = 0.01499093\n",
      "Iteration 80, loss = 0.01433491\n",
      "Iteration 81, loss = 0.01376270\n",
      "Iteration 82, loss = 0.01335647\n",
      "Iteration 83, loss = 0.01292884\n",
      "Iteration 84, loss = 0.01257081\n",
      "Iteration 85, loss = 0.01207984\n",
      "Iteration 86, loss = 0.01163447\n",
      "Iteration 87, loss = 0.01133207\n",
      "Iteration 88, loss = 0.01091151\n",
      "Iteration 89, loss = 0.01057071\n",
      "Iteration 90, loss = 0.01043303\n",
      "Iteration 91, loss = 0.01016277\n",
      "Iteration 92, loss = 0.00958792\n",
      "Iteration 93, loss = 0.00928923\n",
      "Iteration 94, loss = 0.00911488\n",
      "Iteration 95, loss = 0.00862473\n",
      "Iteration 96, loss = 0.00840415\n",
      "Iteration 97, loss = 0.00815512\n",
      "Iteration 98, loss = 0.00793275\n",
      "Iteration 99, loss = 0.00766036\n",
      "Iteration 100, loss = 0.00741643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zb0857\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0, batch_size=5001, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1234, shuffle=True, solver='adam', tol=-10.0,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the network\n",
    "\n",
    "MLP_clf.fit(x_train, y_train0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006959798980621651"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting\n",
    "\n",
    "y_train_hat_clf_sk = MLP_clf.predict_proba(x_train)\n",
    "\n",
    "log_loss(y_train0, y_train_hat_clf_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_train0[0])\n",
    "print(y_train0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.30383052e-13 3.21504558e-12 3.54627091e-10 2.87004393e-03\n",
      " 1.71687203e-20 9.97129956e-01 4.86017969e-16 2.73805068e-11\n",
      " 1.83986776e-12 1.42012103e-10]\n",
      "[9.99998630e-01 1.59786641e-11 1.33555776e-06 2.81816908e-12\n",
      " 2.88471499e-19 1.52875228e-12 1.17987721e-09 1.71159909e-12\n",
      " 1.16440658e-13 3.37065415e-08]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_hat_clf_sk[0])\n",
    "print(y_train_hat_clf_sk[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers:  3\n",
      "Weights layer 1:  78500\n",
      "Weights layer 2:  10100\n",
      "Weights layer 3:  1010\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Layers: \", len(MLP_clf.coefs_))\n",
    "print(\"Weights layer 1: \", MLP_clf.coefs_[0].size+MLP_clf.intercepts_[0].size)\n",
    "print(\"Weights layer 2: \", MLP_clf.coefs_[1].size+MLP_clf.intercepts_[1].size)\n",
    "print(\"Weights layer 3: \", MLP_clf.coefs_[2].size+MLP_clf.intercepts_[2].size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax\n",
      "multiclass\n"
     ]
    }
   ],
   "source": [
    "print(MLP_clf.out_activation_)\n",
    "print(MLP_clf._label_binarizer.y_type_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

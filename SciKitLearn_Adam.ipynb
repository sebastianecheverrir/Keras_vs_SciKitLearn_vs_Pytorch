{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in my Windows the optimizers are located in\n",
    "#/c/Users/zb0857/AppData/Local/Continuum/anaconda2/envs/Tensorflow/lib/site-packages/tensorflow/python/keras\n",
    "#C:\\Users\\zb0857\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\Lib\\site-packages\\tensorflow\\python\\keras\n",
    "#/c/Users/zb0857/AppData/Local/Continuum/anaconda2/envs/Tensorflow/lib/site-packages/sklearn/neural_network\n",
    "#C:\\Users\\zb0857\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\Lib\\site-packages\\sklearn\\neural_network\n",
    "#rememmber to start the notebook using the environment tensorflow\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "import numpy as np\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.keras.optimizers import Fire\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the input data\n",
    "\n",
    "#loading the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "#Separating into train and test (60000 train, 10000 test)\n",
    "(x_train, y_train0),(x_test, y_test0) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#flattening the images (from 28x28 to 784)\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "#we do not need to convert the outputs to one hot vectors.\n",
    "#Also, if we convert the output to one hot vectors, the _label_binarizer.y_type_ \n",
    "#    becomes multilabel and the out_activation_ becomes logistic\n",
    "#    Since keras and pytorch use softmax, we need to keep it like this\n",
    "# #converting the otputs (labels) into one hot vectors\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "# lb.fit(np.array([0,1,2,3,4,5,6,7,8,9]))\n",
    "# y_train = lb.transform(y_train0)\n",
    "# y_test = lb.transform(y_test0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the input parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the input parameters\n",
    "BatchSize         = 6000\n",
    "NeuronsLayer1     = 100\n",
    "NeuronsLayer2     = 100\n",
    "Epochs            = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model in SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the net\n",
    "# Construct our model, # Construct our loss function and an Optimizer. \n",
    "\n",
    "#MLP regressor uses the square loss\n",
    "#MLP classifier uses the log loss function\n",
    "MLP_clf =  MLPClassifier( hidden_layer_sizes         = (NeuronsLayer1,NeuronsLayer2),\n",
    "                          activation                 = 'relu',#,'identity', 'tanh', 'relu'],\n",
    "                          solver                     = 'adam',\n",
    "                          alpha                      = 0.0         ,\n",
    "                          batch_size                 = BatchSize,\n",
    "                          max_iter                   = Epochs              ,\n",
    "                          random_state               = 1234,\n",
    "                          tol                        = -10.0 ,\n",
    "                          verbose                    = True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0,\n",
       " 'batch_size': 6000,\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100, 100),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_iter': 100,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1234,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': -10.0,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': True,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show aprameters of the network\n",
    "MLP_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.90279172\n",
      "Iteration 2, loss = 1.06565240\n",
      "Iteration 3, loss = 0.59918441\n",
      "Iteration 4, loss = 0.42124131\n",
      "Iteration 5, loss = 0.34761391\n",
      "Iteration 6, loss = 0.30541380\n",
      "Iteration 7, loss = 0.27745065\n",
      "Iteration 8, loss = 0.25629642\n",
      "Iteration 9, loss = 0.23928197\n",
      "Iteration 10, loss = 0.22443149\n",
      "Iteration 11, loss = 0.21160107\n",
      "Iteration 12, loss = 0.19918207\n",
      "Iteration 13, loss = 0.18807871\n",
      "Iteration 14, loss = 0.17776680\n",
      "Iteration 15, loss = 0.16964289\n",
      "Iteration 16, loss = 0.16116274\n",
      "Iteration 17, loss = 0.15382683\n",
      "Iteration 18, loss = 0.14766859\n",
      "Iteration 19, loss = 0.14073695\n",
      "Iteration 20, loss = 0.13495804\n",
      "Iteration 21, loss = 0.12909995\n",
      "Iteration 22, loss = 0.12420513\n",
      "Iteration 23, loss = 0.11936994\n",
      "Iteration 24, loss = 0.11408284\n",
      "Iteration 25, loss = 0.10980653\n",
      "Iteration 26, loss = 0.10577001\n",
      "Iteration 27, loss = 0.10164824\n",
      "Iteration 28, loss = 0.09749832\n",
      "Iteration 29, loss = 0.09453175\n",
      "Iteration 30, loss = 0.09066245\n",
      "Iteration 31, loss = 0.08694433\n",
      "Iteration 32, loss = 0.08421069\n",
      "Iteration 33, loss = 0.08207231\n",
      "Iteration 34, loss = 0.07888108\n",
      "Iteration 35, loss = 0.07620255\n",
      "Iteration 36, loss = 0.07369083\n",
      "Iteration 37, loss = 0.07120288\n",
      "Iteration 38, loss = 0.06873556\n",
      "Iteration 39, loss = 0.06669703\n",
      "Iteration 40, loss = 0.06479394\n",
      "Iteration 41, loss = 0.06278767\n",
      "Iteration 42, loss = 0.06033044\n",
      "Iteration 43, loss = 0.05888235\n",
      "Iteration 44, loss = 0.05693785\n",
      "Iteration 45, loss = 0.05553455\n",
      "Iteration 46, loss = 0.05392231\n",
      "Iteration 47, loss = 0.05250410\n",
      "Iteration 48, loss = 0.05099827\n",
      "Iteration 49, loss = 0.04901226\n",
      "Iteration 50, loss = 0.04794999\n",
      "Iteration 51, loss = 0.04618980\n",
      "Iteration 52, loss = 0.04505053\n",
      "Iteration 53, loss = 0.04405929\n",
      "Iteration 54, loss = 0.04292622\n",
      "Iteration 55, loss = 0.04136027\n",
      "Iteration 56, loss = 0.04070013\n",
      "Iteration 57, loss = 0.03903670\n",
      "Iteration 58, loss = 0.03789117\n",
      "Iteration 59, loss = 0.03731433\n",
      "Iteration 60, loss = 0.03589964\n",
      "Iteration 61, loss = 0.03499904\n",
      "Iteration 62, loss = 0.03381636\n",
      "Iteration 63, loss = 0.03301391\n",
      "Iteration 64, loss = 0.03189531\n",
      "Iteration 65, loss = 0.03101315\n",
      "Iteration 66, loss = 0.03030386\n",
      "Iteration 67, loss = 0.02931210\n",
      "Iteration 68, loss = 0.02870461\n",
      "Iteration 69, loss = 0.02814290\n",
      "Iteration 70, loss = 0.02734138\n",
      "Iteration 71, loss = 0.02649279\n",
      "Iteration 72, loss = 0.02560443\n",
      "Iteration 73, loss = 0.02495004\n",
      "Iteration 74, loss = 0.02422085\n",
      "Iteration 75, loss = 0.02391539\n",
      "Iteration 76, loss = 0.02285955\n",
      "Iteration 77, loss = 0.02231099\n",
      "Iteration 78, loss = 0.02184529\n",
      "Iteration 79, loss = 0.02108476\n",
      "Iteration 80, loss = 0.02043789\n",
      "Iteration 81, loss = 0.01964490\n",
      "Iteration 82, loss = 0.01925243\n",
      "Iteration 83, loss = 0.01869374\n",
      "Iteration 84, loss = 0.01835286\n",
      "Iteration 85, loss = 0.01742646\n",
      "Iteration 86, loss = 0.01709117\n",
      "Iteration 87, loss = 0.01671347\n",
      "Iteration 88, loss = 0.01632063\n",
      "Iteration 89, loss = 0.01592855\n",
      "Iteration 90, loss = 0.01584436\n",
      "Iteration 91, loss = 0.01519589\n",
      "Iteration 92, loss = 0.01445967\n",
      "Iteration 93, loss = 0.01406994\n",
      "Iteration 94, loss = 0.01389304\n",
      "Iteration 95, loss = 0.01328076\n",
      "Iteration 96, loss = 0.01292708\n",
      "Iteration 97, loss = 0.01266879\n",
      "Iteration 98, loss = 0.01232828\n",
      "Iteration 99, loss = 0.01206905\n",
      "Iteration 100, loss = 0.01150046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zb0857\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0, batch_size=6000, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1234, shuffle=True, solver='adam', tol=-10.0,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the network\n",
    "\n",
    "MLP_clf.fit(x_train, y_train0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011073467689758523"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting\n",
    "\n",
    "y_train_hat_clf_sk = MLP_clf.predict_proba(x_train)\n",
    "\n",
    "log_loss(y_train0, y_train_hat_clf_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_train0[0])\n",
    "print(y_train0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.72441411e-13 8.80843037e-11 5.17130754e-09 1.96448133e-03\n",
      " 8.96573738e-19 9.98035513e-01 2.08419370e-14 3.49068650e-10\n",
      " 7.21175482e-12 3.59440323e-10]\n",
      "[9.99996185e-01 9.07966350e-11 3.61985781e-06 1.68647723e-10\n",
      " 1.78511302e-16 1.56343376e-11 7.52825274e-09 1.77666502e-11\n",
      " 3.38206528e-12 1.87654302e-07]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_hat_clf_sk[0])\n",
    "print(y_train_hat_clf_sk[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers:  3\n",
      "Weights layer 1:  78500\n",
      "Weights layer 2:  10100\n",
      "Weights layer 3:  1010\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Layers: \", len(MLP_clf.coefs_))\n",
    "print(\"Weights layer 1: \", MLP_clf.coefs_[0].size+MLP_clf.intercepts_[0].size)\n",
    "print(\"Weights layer 2: \", MLP_clf.coefs_[1].size+MLP_clf.intercepts_[1].size)\n",
    "print(\"Weights layer 3: \", MLP_clf.coefs_[2].size+MLP_clf.intercepts_[2].size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax\n",
      "multiclass\n"
     ]
    }
   ],
   "source": [
    "print(MLP_clf.out_activation_)\n",
    "print(MLP_clf._label_binarizer.y_type_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
